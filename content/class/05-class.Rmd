---
title: "Part 5: Data summarizing, reshaping, and wrangling with multiple tables"
linktitle: "Part 5: Data summarizing, reshaping, and wrangling with multiple tables"
date: "2022-02-02"
class_date: "2022-02-02"
output:
  blogdown::html_page:
    toc: true
menu:
  class:
    parent: Class sessions
    weight: 5
type: docs
weight: 5
# pdf: 
# thumb: 
editor_options: 
  chunk_output_type: console
---


## R Project files

Please download the part5 subfolder in [this folder link](https://www.dropbox.com/sh/q4z72523kl84ywp/AAASb1YIFRpxzrjOeq1FVcXua?dl=0) Be sure to unzip if necessary. "Knit" the `code/part5.Rmd` file to install packages and make sure everything is working with data loading.

## Class Video

<iframe width="560" height="315" src="https://www.youtube.com/embed/_V7cKw39Fzc" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>



View last year's class and materials [here](https://sph-r-programming.netlify.app/class/05-class/).



## Slides

During "Muddiest  Parts" review, we will go over [these slides](https://sph-r-programming-2022.netlify.app/slides/05-saving-objects-vs-summaries)

```{r echo=FALSE}
knitr::include_url('https://sph-r-programming-2022.netlify.app/slides/05-saving-objects-vs-summaries#1')
```

## Another useful video

[Dr. Kelly Bodwin's Reshaping Data Video](https://youtu.be/4ZIFb1BoIB4)

For a short version, watch the [pivot_longer excerpt](https://youtu.be/4ZIFb1BoIB4?t=365) about "working backwards" from a plot. Then watch the [pivot_wider excerpt](https://youtu.be/4ZIFb1BoIB4?t=620)

## Post-Class

Please fill out the following survey and we will discuss the results during the next lecture. All responses will be anonymous.

- Clearest Point: What was the most clear part of the lecture?
- Muddiest Point: What was the most unclear part of the lecture to you?
- Anything Else: Is there something you'd like me to know?

<https://forms.gle/4tVx1mL7SzQx7MCu5>


## Muddiest Points

> fct_other() and fct_collapse: I actually think I understand the point of each of these separately, but somehow using them together like in the example totally confused me. I'll have to go look at it again later. 

> The data cleaning section where we used forcats since it was confusing as to how to use it.

> when to use fct_collapse vs case when and when you would want to turn a character into a vector

Thanks for this, I went through this too quickly it seems. First, I should say that the two ways of creating the `cigarettes10` categorical vector -- using `case_when()` or using `forcats` factor function -- are both equally good options. You can always turn a character vector into a factor after using case_when. I will show some benefits of a factor vector in part 6 and also when we talk about statistical modeling. Here is a visual explanation of what I was doing with `fct_other()` + `fct_collapse()`:

<img src = "/img/cigarettes10_forcats.png">



> Also the challenge question to change the logical "ever_been_bullied" into a character vector with ifelse() ... I have never used ifelse() and that made zero sense to me. Will have to play with it to understand.

Good point, I showed this mainly because it's another option (and one I default to using sometimes) but didn't carefully go through it, sorry about that. You can always just use `case_when()`, but if you want to learn more about the base R function `ifelse()`, it is basically a simpler `case_when` with only one condition, and two possible values. The way we use it is like

`ifelse(BOOLEAN_TEST_CONDITION, value_if_test_is_TRUE, value_if_test_is_FALSE)`

For instance:

```{r message = FALSE}
library(tidyverse)

# simple test
1==2 # this is a boolean test
ifelse(1==2, "math is wrong", "math is right")
ifelse(1==1, "math is wrong", "math is right")

# ifelse() is vectorized
myvec <- c(1,2,NA)

ifelse(is.na(myvec), "missing", "not missing")

ifelse(myvec==2, "two", "not two") # NAs are preserved

# we can save the resulting vector
newvec <- ifelse(myvec==2, "two", "not two") 
newvec

# we can use it in mutate as well
# make a small data set
mydata <- head(mtcars[,1:4])
mydata

mydata %>% mutate(
  cyl6 = ifelse(cyl==6, "cyl is 6!!!", "cyl is not 6")
)

```



> adorn_percentages: On the species/sex tabyl example, percentages were calculated across rows for sex. Is there a way to change percentages to calculate down columns for species? 

Yes!! When I talk more about tables, statistical modeling, etc I was planning to go more in depth with janitor tabyls. But it's so useful I can't help but keep showing you tabyl functions along the way. So here's some more tips:

```{r message = FALSE}
library(janitor)
library(palmerpenguins)
library(gt)

# simple cross table with counts
penguins %>% 
  tabyl(species, sex) 

# simple cross table with percents (denominator is row by default)
penguins %>% 
  tabyl(species, sex) %>%
  adorn_percentages()

penguins %>% 
  tabyl(species, sex) %>%
  # instead of counts, show percentages, the default denominator is row
  adorn_percentages(denominator = "col") 

penguins %>% 
  tabyl(species, sex) %>%
  adorn_percentages(denominator = "col") %>%
  # add row and column totals, the default is to show just column totals in "row"
  # this makes for a strange row total, though, as we get 300%
  adorn_totals(where = c("row", "col")) 

penguins %>% 
  tabyl(species, sex) %>%
  adorn_percentages(denominator = "col") %>%
  adorn_totals(where = c("row", "col")) %>%
  # need to have adorn_totals BEFORE adding pct formatting, otherwise error
  adorn_pct_formatting()


penguins %>% 
  tabyl(species, sex) %>%
  adorn_percentages(denominator = "col") %>%
  adorn_totals(where = c("row", "col")) %>%
  adorn_pct_formatting() %>%
  # add back in counts in ()
  # need to have adorn_ns AFTER adding pct formatting, otherwise error
  adorn_ns()


penguins %>% 
  tabyl(species, sex) %>%
  # percent of total (denominator is the total sum)
  adorn_percentages(denominator = "all") %>%
  # now it makes more sense to have totals in both row and column
  adorn_totals(where = c("row", "col")) %>%
  adorn_pct_formatting() %>%
  adorn_ns()

penguins %>% 
  tabyl(species, sex) %>%
  adorn_percentages(denominator = "all") %>%
  adorn_totals(where = c("row", "col")) %>%
  adorn_pct_formatting() %>%
  adorn_ns() %>%
  # add title, need placement = "combined" for gt to work
  adorn_title(placement = "combined") %>%
  # make it fancy html
  gt()

penguins %>% 
  tabyl(species, sex) %>%
  adorn_percentages(denominator = "all") %>%
  adorn_totals(where = c("row", "col")) %>%
  adorn_pct_formatting() %>%
  adorn_ns() %>%
  # make it fancy html
  # specify that species denotes the name of the rows (removes that column label)
  # adds line between row names and rest of table
  gt::gt(rowname_col = "species") %>%
  # adds back in Species above rows
  gt::tab_stubhead(
    label = "Species"
  ) %>%
  # adds header
  gt::tab_header(
    title = "Species by sex percentages (counts)",
    subtitle = "Palmer penguin data"
  ) %>%
  # adds sex label across multiple columns
  gt::tab_spanner(
    label = "Sex",
    columns = c(female, male, `NA_`)
  )
```

See the [intro to gt package for more examples like this](https://gt.rstudio.com/articles/intro-creating-gt-tables.html).


> sort and deduplicate prior to joining tables: Would you mind sharing an example of how to do this to avoid the multiple key warning when merging tables?

Yes, I will first say that sometimes you *want* duplicate keys in your end result, which we will see in part 6 example. For instance, if you are joining a study cohort data set with longitudinal lab values in tidy "long" format. 

But you do want to avoid having duplicate keys in both the left and right table, as this will cause chaos and extreme duplication of rows.

```{r}
example_cohort <- tibble(
  name = c("jane", "juan", "jessica", "jessica"),
  value = c("a", "b", "c", "c"),
)

example_other <- tibble(
  name = c("juan", "juan","jessica"),
  y = c(3, 1, 4)
)

example_cohort
example_other

# with duplicate rows in the left and right data, 
# we end up with duplicated rows in the full data
left_join(example_cohort, 
          example_other)

# these do the same thing, since only one matching key with the same "name"
left_join(example_cohort, 
          example_other,
          by = "name")

left_join(example_cohort, 
          example_other,
          by = c("name" = "name"))

# perhaps the example_cohort was a mistake, 
# and we want to remove those duplicated rows first:
example_cohort %>% distinct()

left_join(
  example_cohort %>% distinct(),
  example_other
)

# the above assumes that juan had two y values on purpose

# what if we didn't want two juan values in the "example_other" tibble
# let's assume we want only the lowest y value in example_other for each name
example_other %>%
  group_by(name) %>%
  slice_min(y)

left_join(
  example_cohort %>% distinct(),
  example_other %>% group_by(name) %>% slice_min(y)
)

# Here's a reminder about what inner join and full join would do in the original situation:

# same as left join in this case, takes all data from both tables
full_join(example_cohort, 
          example_other)

# only uses names (the joining key) from both tables
inner_join(example_cohort, 
          example_other)
```

> Do we need to save all of our graphics that we make in our homework? like using ggsave() and a png file? Same with our glimpse/skim/summary bits that we code?

You don't need to unless I specifically state "save as a file" (i.e. with `ggsave()` or `write_csv()`) or "save as an object" (i.e. with `myplot <- ggplot(.....` or `mysummary <- summary(mydata)`) depending on the setting.

> Thank you for getting into "messy" data. I still would like more information about primary data.

I have to admit I am not sure what you mean by "primary" data, because to me, this could mean an infinite number of things. It depends on the type of data and who is entering the data: is it coming from a person or a machine? Is it an export from Redcap, an EHR query, or an excel sheet where a person has entered the data by hand? 

I would love to hear more about what kind of "primary" data people are interested in. Please send another survey message or comment on my question on slack!

In part 6 I have used a "real" data example, basically this is the most common type of primary data that I receive as a statistician -- an excel sheet where someone in a lab has input data, copied some omics data from another excel sheet into that table, and sent it to me via email. This type of data can lend itself to lots of strange formatting issues, and also lots of data input error. It is not ideal, but this is where we're at in science (see again [Broman & Woo 2017, Data Organization in Spreadsheets](https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1375989) or [Ellis & Leek 2018, How to share data for collaboration](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7518408/)).

I also need to teach you the basic tools for cleaning and manipulating data before I can get you started with "real" data in super messy format, because otherwise it might be overwhelming doing a series of steps! But I think you're almost ready! You'll see how the data cleaning steps can really pile on today during the modestly messy example. Most of my "data cleaning" or "importing" or "wrangling" tasks as a statistician involve dealing with weird spreadsheet formats, typos, missing values, and then creating new variables that I need for analyses or graphing. I think you've learned most of those tasks by now in this class. In part 6 we will practice these more. In future classes we will learn more about how to handle dates/times, more about string manipulation, reshaping data, and additional ways to deal with missing data.

If anyone has primary data they would like me to go over in class (and can share it), please send it to me! Or describe the data you have in mind.

There was a helpful suggestion to show data with the kind of demographic or survey variables that are somewhat difficult to keep cleanly categorized, but that are very important, such as sexual identity, race/ethnicitiy, disability, etc. I perused some of the publicly available data out there but haven't found a good enough example yet; I will keep looking, but if you know of one send it my way, please!


> If we have 100 cvs to import, how to import data quickly?

The best way is to use lists and `purrr`! We will hopefully get to lists by class 7 and `purrr` probably class 8. It also depends on whether these 100 csv files are in the same format and what you want to do with them after you import them. Are they able to be stacked, so you can use `bind_rows()`? If so, it would be something like this:

```{r}
# have all the csvs in one place
folder_where_csvs_are <- here::here("static/data")
# we can list all their names
list.files(folder_where_csvs_are, pattern = ".csv")
# list all their names will full file path
all_csv_files <- list.files(here::here("static/data"), pattern = ".csv", full.names = TRUE)
# use purrr map to load them in
list_of_data <- purrr::map(all_csv_files, ~read_csv(.))
# list of data frames
class(list_of_data)
# length of list
length(list_of_data)
# see the structure of the data
# str(list_of_data) # this is long output
# first element
glimpse(list_of_data[[1]])
# if they were all the same columns and able to stack, we could do this
# big_data_frame <- bind_rows(list_of_data)
```



# Clearest points

> joining data, summarize & group_by (we used these a lot in BSTA511)

good to know, I like hearing about what you have already learned in BSTA511, so keep that comin'

> here () was a lot more clear on re-visiting.

so glad!

> The summarize section was pretty understandable

Glad to hear it, I was worried I rushed that. We will go over it again in class 6 anyway with `across()`.

> I appreciated the overall discussion on muddy points from previous session. This definitely helped.

Good! We're doing another long muddy review today too =)

# Messages to me

> I feel like it would be really helpful (to me anyway) to hear about some stupid-but-useful R tidbits like: how you filter to get any row that doesn't have an NA. Or how you filter to have any row that doesn't have an NA in a particular column. I'm sure there's tons more I haven't thought of -- those are just ones that I've wanted and struggled to come up with on my own in the past.

Yes! I keep meaning to show you more with missing data. I have a couple more examples during class 6 using `drop_na()` to do all these things. Also those are very useful tidbits, sorry I haven't gotten to them yet!

> The challenges are so helpful - great to try out what you are showing!

Great to hear, class 6/part 6 has a toooon of challenges, maybe too many challenges. We will see how it goes =)

> I hope to spend some time on summary tables

Thank you, we will definitely do this, starting with redo-ing the end of part 5 today, which we didn't finish.

> Is it possible to have a list of all the packages we need for each assignment? I did get marked down for adding too many libraries since I keep getting them all jumbled between homework, practices, and out-of-class practice as well

This is a good point, though I don't think you're getting marked down for having too many packages (if I'm wrong let me know!), I think Colin is just giving comments that you should try to avoid it. I didn't think about this when writing homeworks but I will start giving you the packages I think you need. It's hard to keep track of all the different ones when you are just starting out.

Honestly, I don't do a very good job myself of only loading the minimal list of packages in each Rmd. In theory, it doesn't matter too much---it may add some extra time to load the extra packages, but often I am copying my YAML and setup code from other Rmds and forget to take out the packages I don't need.

One issue with loading too many packages is that function names can overlap between packages and cause errors or confusion. For instance, the function `select()` is in both the `tidyverse` or `dplyr` package and the `AnnotationDBI` Bioconductor package which I sometimes use. If I load the `AnnotationDbi` package after I load `dplyr`, R thinks I am trying to use `AnnotationDBI::select` instead of `dplyr::select`. There are ways around it, such as making sure you load `library(dplyr)` last, or when using `select` use `dplyr::select` explicitly in your code instead, or, redefining select this way:

```{r, message = TRUE}
library(dplyr)
library(AnnotationDbi)

select

select <- dplyr::select

select
```


